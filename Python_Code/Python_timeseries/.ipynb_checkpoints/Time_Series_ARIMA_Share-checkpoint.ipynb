{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecast - ARIMA\n",
    "\n",
    "### Kumar Rahul\n",
    "\n",
    "Forecasting the demand of services or products leads to better management of short term or long term planning. In this case, we are looking at the warranty related issues reported, on a particular brand of two-wheeler. The data is a monthly roll-up of approximately half a million issues reported by the customers over a four year period. \n",
    "We will be using Claim forecasting data in this exercise. Refer the **Exhibit 1** to understand the feature list. Use the data and answer the below questions.\n",
    "\n",
    "1.\tLoad the time series dataset in Jupyter Notebook using pandas.\n",
    "2.\tPlot the time series data to visualize trend and seasonality in the data.\n",
    "3.\tDecompose the claim data to report Trend, Seasonality, and irregular component. Find the Seasonality window using the claim data.\n",
    "4.\tTest stationarity of data using augmented dickey fuller test.\n",
    "5.\tUse data differencing as a strategy to make the data stationary.\n",
    "6.\tPlot the ACF and PACF plot. How will you inspect the plot to arrive at the p-lags and q-lags?\n",
    "7.\tSplit the data into training set and test set. Use walk forward validation strategy for model building and evaluation.\n",
    "8.\tGiven recent claim, what is the expected claim for the next time period? Build a model with statsmodel.api to forecast the amount claimed in next time step.\n",
    "9.\tCheck for validity of model using ACF and PACF plot for error term. What do you observe in these plots?\n",
    "10.\t How do you interpret the model outcome? Report the model performance on the walk forward validation set.\n",
    "\n",
    "**Exhibit 1**\n",
    "\n",
    "|Sl. No.|Name of Variable|Variable Description|\n",
    "|----------|------------|---------------|\n",
    "|1\t|date\t|Date of Claim|\n",
    "|2\t|rate\t|Amount claimed|\n",
    "|3\t|item\t|Number of claims|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from math import sqrt\n",
    "from numpy import array\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "monthly_raw_df = pd.read_csv('./Claim_forecasting/data/data_monthly.csv', sep=',', header=0, \n",
    "                             low_memory=False, infer_datetime_format=True, \n",
    "                             index_col=['date'], \n",
    "                             parse_dates= ['date'],dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_raw_df.sort_index(inplace=True)\n",
    "monthly_raw_df.columns = monthly_raw_df.columns.str.lower().str.replace('.', '_')\n",
    "monthly_raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the first day and last day of CGM monitoring being trucated as it has not been captured for the full cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_filter_df = monthly_raw_df.filter(['rate'], axis =1)\n",
    "monthly_filter_df['rate'] = monthly_filter_df['rate'].map(lambda x:str(x).replace(',', '')).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_filter_df = monthly_filter_df[(monthly_filter_df.index >='2014-03-01') & \n",
    "                                      (monthly_filter_df.index <= '2017-05-31')]\n",
    "\n",
    "monthly_filter_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Framing\n",
    "\n",
    "\n",
    "We will use the data to explore a very specific question; that is:\n",
    "\n",
    "**Given recent claim, what is the expected claim for the next time period?**\n",
    "\n",
    "Plot of the original data is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (18, 5))\n",
    "pyplot.plot(monthly_filter_df, 'b-')\n",
    "pyplot.title('Monthly amount claimed over a 3 year period')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Trend, Seasonality, Seasonality window and Test stationarity of data\n",
    "\n",
    "### Trend and Seasonality\n",
    "* **Trends**: The consistent (long term) upward or downward movement of peaks in the data. First the visual inspection of the monthly glucose level. Data seems to show spikes coming down with time. In a way, suggesting that the average glucose level has come down with time (possibly due to the treatment).\n",
    "\n",
    "One of the ways to identify trends is to identify the seasonality window.\n",
    "\n",
    "* **Seasonality window**: The idea of finding seasonality window is to find a window size `s`, for which if the rolling average is calculated for each time point(-s/2<t<s/2), the zigzag motion in the time series data smoothens out. The rolling average in the window `s` tries to smooth out noise and seasonality and what is observed is a pure trend.\n",
    "\n",
    "* **Seasonality**: Seasonality, measured in terms of seasonality index, is fluctuations from the trend that occurs within a defined time period (seasons, quarters, months, days of the week, time interval within a day etc.).\n",
    "\n",
    "    > * Substracting trend from the original data will lead to looking at the seasonal component alone. \n",
    "         1. If it is additive model then detrended data = original data - trend. \n",
    "         2. If it is a multiplicative model, then  detrended data = original data/trend.\n",
    "         3. detrended data is extraploated to fill for the missing value (due to smoothning using seasonality index) at the top and bottom using least sqaured extrapolation. \n",
    "         4. period average is calculated using mean of the detrended data at a defined frequency. Frequency is an integer value that gives the number of periods per cycle. E.g., 12 for monthly. NaNs are ignored in the mean. \n",
    "         5. If additive model, period average = period average - (mean of period average)\n",
    "         6. If multiplcative model, period average = period average/mean of period average.\n",
    "         7. seasonal component is obtained by filling/tiling the period averages across all timeperiods.\n",
    "         8. If additive model,  resid = x / seasonal / trend\n",
    "         9. If multiplicative model,  resid = original data - seasonal - trend\n",
    "         Source: https://github.com/statsmodels/statsmodels/blob/master/statsmodels/tsa/seasonal.py\n",
    "\n",
    "    > * The other way to remove trend and look at the seasonal component alone is to perform differencing.\n",
    "\n",
    "\n",
    "Seasonality is formally defined as correlational dependency of order k between each i'th element of the series and the (i-k)'th element and measured by autocorrelation (i.e., a correlation between the two terms); k is usually called the lag. \n",
    "\n",
    "If the measurement error is not too large, seasonality can be visually identified in the series as a pattern that repeats every k elements. \n",
    "\n",
    "Seasonality for a particular lag of k can be removed by differencing the series, that is converting each i'th element of the series into its difference from the (i-k)''th element. There are two major reasons for such transformations.\n",
    "\n",
    "> 1. We can identify the hidden nature of seasonal dependencies in the series.\n",
    "2. The other reason for removing seasonal dependencies is to make the series stationary which is necessary for ARIMA and other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decompose the series to analyze the underlying Trend and Seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decomfreq = ((24*60)//15) #96 observations in a day.\n",
    "decomposition = sm.tsa.seasonal_decompose(monthly_filter_df, model='additive') #,freq = decomfreq)\n",
    "\n",
    "dplot = decomposition.plot()\n",
    "dplot.set_figwidth(20)\n",
    "dplot.set_figheight(10)\n",
    "dplot.suptitle('Decomposition of time series - monthly claim data')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function is being used for identifying the trend by computing the seasonality window. With hit and trial the seasonality window seems to be 13. The rolling mean in the below chart and the trend component above seems to be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_stats(data, roll):\n",
    "    #Determing rolling statistics\n",
    "    rolmean = data.rolling(window=roll, center = True).mean()\n",
    "    rolstd = data.rolling(window=roll, center = True).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    pyplot.figure(figsize = (18, 5))\n",
    "    orig = pyplot.plot(data, color='blue',label='Original')\n",
    "    mean = pyplot.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = pyplot.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    pyplot.legend(loc='best')\n",
    "    pyplot.title('Rolling Mean & Standard Deviation')\n",
    "    pyplot.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_stats(monthly_filter_df,13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreating the seasonality component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_trend_df = monthly_filter_df.rolling(window=13, center = True).mean()\n",
    "detrended = monthly_filter_df-decomposition.trend\n",
    "period_average = detrended.groupby(detrended.index.month).mean()\n",
    "overall_period_average = period_average.mean()\n",
    "seasonal_component = period_average-overall_period_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the value of `seasonal_component` from month of september to decemeber with the value given by `decompose.seasonal`. The `decompose.residual` is a set of data point which can be used further for model building. The `trend` and `seasonal` componenet will be added back to the predicted value obtained after making a time series model on the `residual`.\n",
    "\n",
    "### Additive Model\n",
    "Y_Predicted = **decomposition.trend** + **decomposition.seasonal** + **decomposition.resid**\n",
    "\n",
    "### Multiplicative Model\n",
    "Y_Predicted = **decomposition.trend** * **decomposition.seasonal** * **decomposition.resid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Rolling Statistics as depicted above is more of a visual inspection. It might not always be possible to identify trend and seasonality by using such visual inferences. A test for stationarity of the series will be better if the objective is to use the data to forecast the future outcome.\n",
    "\n",
    "### Stationarity\n",
    "We can assume the series to be stationary if it has constant statistical properties over time, i.e. the following:\n",
    "\n",
    "> 1. constant mean - The mean of the series should not be a function of time rather should be a constant. \n",
    "2. constant variance - The variance of the series should not a be a function of time. This property is known as homoscedasticity. \n",
    "3. an autocovariance that does not depend on time - The covariance of the i th term and the (i + m) th term should not be a function of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, we can check stationarity using the following:\n",
    "\n",
    "> Dickey-Fuller Test: This is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the ‘Test Statistic’ is less than the ‘Critical Value’, we can reject the null hypothesis and say that the series is stationary.\n",
    "\n",
    "\n",
    "The below function which takes a timeseries data as input and perform the Dickey-Fuller test to check stationarity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def df_test(data):\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(data.iloc[:,0].values, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test(monthly_filter_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-vlaue is not greater than the 5% or 10% cofidence level, suggests that the timeseries is stationary. As the series is statioanry, differencing may not be required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make time series stationary\n",
    "\n",
    "There are 2 major reasons behind non-stationarity of a time series data:\n",
    "\n",
    "> * Trend – varying mean over time. In this case we can see that on average, the claim is varying  over time.\n",
    "* Seasonality – variations at specific time-frames. In this case, claim might be high during some time period owing to climatic conditions leading to wear and tear.\n",
    "\n",
    "\n",
    "The underlying principle is to estimate the trend and seasonality in the series and remove those from the series to get a stationary series. Then statistical forecasting techniques can be implemented on the filtered series. The final step would be to convert the forecasted values into the original scale by applying trend and seasonality constraints back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and Eliminate Trend and Seasonality\n",
    "\n",
    "The simple trend reduction techniques discussed before don’t work in all cases, particularly the ones with high seasonality. Two widely used techniques of removing trend and seasonality:\n",
    "\n",
    "> * Differencing – taking the differece with a particular time lag\n",
    "* Decomposition – modeling both trend and seasonality and removing them from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differencing\n",
    "One of the most common methods of dealing with both trend and seasonality is differencing. In this technique, the difference of the observation at a particular instant with that at the previous instant is taken. This mostly works well in improving stationarity. First order differencing can be done in Pandas as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "monthly_filter_log_df = np.log(monthly_filter_df)\n",
    "monthly_filter_log_diff_df = monthly_filter_log_df - monthly_filter_log_df.shift(1)\n",
    "\n",
    "pyplot.figure(figsize = (18, 5))\n",
    "diff = pyplot.plot(monthly_filter_log_diff_df, color='red',label='Difference')\n",
    "monthly_filter_log_diff_df.dropna(inplace=True)\n",
    "monthly_filter_log_diff_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to have reduced trend considerably. Same is evident in the plot and test below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_rolling_stats(monthly_filter_log_diff_df,13)\n",
    "df_test(monthly_filter_log_diff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try second order and third order differencing to further remove the variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decomposing\n",
    "\n",
    "We used decomposition in earlier section. In this approach, both trend and seasonality are modeled separately and the remaining part of the series is returned.  Split the data to bring out both trend and seasonality. The remaining part of the series i.e. residuals can be modeled. Check the stationarity of `ts_residual` by using the pre-defined functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts_log_decompose = decomposition.resid\n",
    "ts_log_decompose.dropna(inplace=True)\n",
    "\n",
    "plot_rolling_stats(ts_log_decompose, 13)\n",
    "df_test(ts_log_decompose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for this dataset, the time series is stationary we can set d = 0 and will find the value of p and q using ACF and PACF plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation Analysis\n",
    "\n",
    "We can assume the distribution of each variable fits a Gaussian (bell curve) distribution. If this is the case, we can use the Pearson’s correlation coefficient to summarize the correlation between the variables.\n",
    "\n",
    "The Pearson’s correlation coefficient is a number between -1 and 1 that describes a negative or positive correlation respectively. A value of zero indicates no correlation.\n",
    "\n",
    "We can calculate the correlation for time series observations with observations with previous time steps, called lags. Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called a serial correlation, or an autocorrelation.\n",
    "\n",
    "A plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function, or the acronym ACF. This plot is sometimes called a correlogram, or an autocorrelation plot.\n",
    "\n",
    "A partial autocorrelation function or PACF is a summary of the relationship between an observation in a time series with observations at prior time steps with the relationships of intervening observations removed.\n",
    "\n",
    "The autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps.\n",
    "\n",
    "It is these indirect correlations that the partial autocorrelation function seeks to remove. This is the intuition for the partial autocorrelation.\n",
    "\n",
    "We can calculate autocorrelation and partial autocorrelation plots using the plot_acf() and plot_pacf() statsmodels functions respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACF and PACF plot for the monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plots\n",
    "pyplot.figure(figsize = (25, 15))\n",
    "lags = 20\n",
    "# acf\n",
    "axis = pyplot.subplot(2, 1, 1)\n",
    "plot_acf(monthly_filter_df, ax=axis, lags=lags)\n",
    "# pacf\n",
    "axis = pyplot.subplot(2,1,2)\n",
    "plot_pacf(monthly_filter_df, ax=axis, lags=lags)\n",
    "# show plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Sets\n",
    "We will use the first three years of data for training predictive models and the final year for evaluating models.\n",
    "\n",
    "The function split_filter_df() below splits the monthly data into train and test sets and organizes each into standard weeks.\n",
    "\n",
    "Specific row offsets are used to split the data using knowledge of the filter_df. The split filter_dfs are then organized into  data using the NumPy split() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_filter_df(data):\n",
    "    split_point = len(data) - 10\n",
    "    train, test = data[0:split_point], data[split_point:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new file\n",
    "train, test = split_filter_df(monthly_filter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate train data\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate test data\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric\n",
    "\n",
    "Both Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) can be used. Unlike MAE, RMSE is more punishing of forecast errors.\n",
    "\n",
    "The function evaluate_forecasts_rmse() and evaluate_forecasts_mape() is being used for evaluating model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more  forecasts against expected values\n",
    "def evaluate_forecasts_rmse(actual):\n",
    "    score_rmse = 0\n",
    "    se = 0\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[0]):\n",
    "        # calculate mse\n",
    "        se += (actual.iloc[i,0] - actual.iloc[i,1])**2\n",
    "        # calculate rmse\n",
    "    score_rmse = sqrt(se/actual.shape[0])\n",
    "    return score_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more  forecasts against expected values\n",
    "def evaluate_forecasts_mape(actual):\n",
    "    score_mape = 0\n",
    "    ape = 0\n",
    "    for i in range(actual.shape[0]):\n",
    "        # calculate mse\n",
    "        ape += np.abs(((actual.iloc[i,0] - actual.iloc[i,1])/actual.iloc[i,0]))\n",
    "        # calculate mape\n",
    "    score_mape = (ape)/actual.shape[0]\n",
    "    return actual, score_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk-Forward Validation\n",
    "Models will be evaluated using a scheme called walk-forward validation.\n",
    "\n",
    "This is where a model is required to make a one month prediction, then the actual data for that month is made available to the model so that it can be used as the basis for making a prediction on the subsequent month. This is both realistic for how the model may be used in practice and beneficial to the models, allowing them to make use of the best available data.\n",
    "\n",
    "A walk-forward validation, or rolling forecast, method is used as follows:\n",
    "\n",
    "> * Each time step in the test dataset is iterated.\n",
    "* Within each iteration, a new ARIMA model is trained on all available historical data.\n",
    "* The model is used to make a prediction for the next time step.\n",
    "* The prediction is stored and the “real” observation is retrieved from the test set and added to the history for use in the next iteration.\n",
    "* The performance of the model is summarized at the end by calculating the root mean squared error (RMSE) of all predictions made compared to expected values in the test dataset.\n",
    "\n",
    "\n",
    "The name of a function is provided for the model as the argument “model_func“. This function is responsible for defining the model, fitting the model on the training data, and making a one-week forecast.\n",
    "\n",
    "The forecasts made by the model are then evaluated against the test dataset using the previously defined evaluate_forecasts() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(model_func, train, test, order):\n",
    "    # history is a list of  data\n",
    "    history = train.filter(['rate'], axis = 1)\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = model_func(history, order)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)      \n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history = history.append(test.iloc[[i]])\n",
    "    predictions = array(predictions)\n",
    "    test['prediction'] = predictions\n",
    "    # evaluate predictions days for each week\n",
    "    actual, score_mape = evaluate_forecasts_mape(test[:])\n",
    "    score_rmse = evaluate_forecasts_rmse(test[:])\n",
    "    return actual, score_mape, score_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop an Autoregression Model\n",
    "We can develop an autoregression model for univariate series of monthly claims.\n",
    "\n",
    "The Statsmodels library provides multiple ways of developing an AR model, such as using the AR, ARMA, ARIMA, and SARIMAX classes.\n",
    "\n",
    "We will use the ARIMA implementation as it allows for easy expandability into differencing and moving average.\n",
    "\n",
    "First, the history data comprised of weeks of prior observations must be converted into a univariate time series of monthly power consumption. We can use the to_series() function developed in the previous section.\n",
    "\n",
    "Note: To make holts winter model, use:\n",
    "\n",
    "> * `import statsmodels.tsa.holtwinters as hw`\n",
    "* `model = hw.ExponentialSmoothing()`\n",
    "\n",
    "Help on the model - hw.ExponentialSmoothing.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arima forecast\n",
    "def arima_forecast(history, pdq):\n",
    "    # define the model\n",
    "    model = ARIMA(history, order=pdq)\n",
    "    # fit the model\n",
    "    model_fit = model.fit(trend = 'nc',disp=0)\n",
    "    # make forecast\n",
    "    yhat = model_fit.forecast(steps=1)[0]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_column(test):\n",
    "    for n in test.columns:\n",
    "        if n =='prediction':\n",
    "            test.drop('prediction', axis = 1, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define the names and functions for the models we wish to evaluate\n",
    "models = dict()\n",
    "models['arima'] = arima_forecast\n",
    "\n",
    "import itertools\n",
    "p_values = [0,1,2]\n",
    "d_values  = [0, 1]\n",
    "q_values = [0, 1, 2]\n",
    "\n",
    "pdq = list(itertools.product(p_values, d_values, q_values))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# evaluate each model\n",
    "for name, func in models.items():\n",
    "    best_mape,best_rmse, best_cfg = float(\"inf\"), float(\"inf\"), None\n",
    "    for pdq in pdq:\n",
    "        order = pdq\n",
    "        del_column(test)\n",
    "        try:\n",
    "            actual, score_mape, score_rmse= evaluate_model(func, train, test, pdq)\n",
    "            if score_rmse < best_rmse:\n",
    "                best_rmse, best_mape, best_cfg = score_rmse, score_mape, order\n",
    "            print('ARIMA%s MAPE=%.5f RMSE=%.4f' % (order,score_mape,score_rmse))\n",
    "        except:\n",
    "            continue\n",
    "    print('Best ARIMA%s MAPE=%.5f RMSE=%.5f' % (best_cfg,best_mape, best_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()\n",
    "models['arima'] = arima_forecast\n",
    "\n",
    "# evaluate each model\n",
    "for name, func in models.items():\n",
    "    # evaluate and get scores\n",
    "    # print(models.items())\n",
    "    #score_mape, scores_mape, score_rmse, scores_rmse = evaluate_model(func, train, test)\n",
    "    pdq = (1,0,0)\n",
    "    del_column(test)\n",
    "    actual, score_mape, score_rmse= evaluate_model(func, train, test, pdq)\n",
    "    print('ARIMA%s MAPE=%.5f RMSE=%.4f' % (pdq,score_mape,score_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual['converted_rate'] = np.exp(actual.rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual['converted_forecast'] = np.exp(actual.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ape = 0\n",
    "se=0\n",
    "for i in range(actual.shape[0]):\n",
    "        # calculate mse\n",
    "        ape += np.abs(((np.exp(actual.iloc[i,0]) - np.exp(actual.iloc[i,1]))/np.exp(actual.iloc[i,0])))\n",
    "        # calculate mape\n",
    "        se += (np.exp(actual.iloc[i,0]) - np.exp(actual.iloc[i,1]))**2\n",
    "        # calculate rmse\n",
    "                       \n",
    "score_mape = (ape)/actual.shape[0]\n",
    "score_rmse = sqrt(se/actual.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual['error']= actual.rate-actual.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "pyplot.figure(figsize = (25, 15))\n",
    "lags = 9\n",
    "# acf\n",
    "axis = pyplot.subplot(2, 1, 1)\n",
    "plot_acf(actual.error, ax=axis, lags=lags)\n",
    "# pacf\n",
    "axis = pyplot.subplot(2,1,2)\n",
    "plot_pacf(actual.error, ax=axis, lags=lags)\n",
    "# show plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual.to_csv('monthly_forecast.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
