{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"font-family: Arial; font-size:3.75em;color:purple; font-style:bold\"><br>\n",
    "Data Analysis - H2O models:\n",
    "</p><br>\n",
    "\n",
    "## By Kumar Rahul\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#To know the environment with the python kernal\n",
    "import sys, os\n",
    "\n",
    "sys.executable\n",
    "\n",
    "## to open the notebook in presentation mode.\n",
    "\n",
    "#jupyter nbconvert .ipynb --to slides --post serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We are going to use below mentioned libraries for **data import, processing and visulization**. As we progress, we will use other specific libraries for model building and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "libraries, echo=TRUE, message=FALSE, warning=FALSE",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import h2o as h2o\n",
    "h2o.init()\n",
    "import seaborn as sn # visualization library based on matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "#the output of plotting commands is displayed inline within Jupyter notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Run a R code in python kernal\n",
    "\n",
    "To run R code from within python kernal. use pip to install rpy2 (if not already installed). (!pip install rmagic)\n",
    "\n",
    "Use conda not pip if the R installation is done using conda (!conda install -c r rpy2)\n",
    "\n",
    "The rmagic function has moved to rpy2 and thus the installation of rpy2 is needed. Once done, use the below code to load rpy2.ipython and follow with the code. YOu willl find a note on using %load_ext rmagic but this does not work now.\n",
    "\n",
    "#!pip install rmagic\n",
    "#!conda install -c r rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%R setwd('/Users/Rahul/Documents/Rahul Office/IIMB/Projects @ IIMB/Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R .libPaths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The python command to get the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Data Import and Manipulation\n",
    "\n",
    "### 1. Importing a data set\n",
    "\n",
    "This analysis is for customer feedback data on various products used by the customers over a period of time. The feedback was collected between 2017-2018 by the customer care and support division of a company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Modify the ast_note_interactivity kernel option to see the value of multiple statements at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In case the file is not getting read, probably the utf-8 encoding is not correct.\n",
    "\n",
    "Open csv file in notepad++ and change the encoding throught Encoding menu -> convert to UTF-8. Then saving the file. Then again running python program over it.\n",
    "\n",
    "Reading the file through `pandas` and then converting it to `h2o` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "readData, echo=TRUE,tidy=TRUE",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv( \"\", \n",
    "                        sep = ',', na_values = ['', ' '])\n",
    "raw_df.columns = raw_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "#raw_df.head()\n",
    "\n",
    "#raw_df.columns\n",
    "raw_h2f = h2o.H2OFrame(raw_df)\n",
    "raw_h2f.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The data set is baised with 53% promotor, 43% passive and only 4% detractors. Traditional models may not yield good results in classifying detractors.\n",
    "\n",
    "Re-grouping Passive to Detractors with an assumption that Passive customers may switch side but if views of these customers tends towards being detractors, the word of mouth (WOM) may not be good for the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "readData, echo=TRUE,tidy=TRUE",
    "autoscroll": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "raw_h2f = h2o.upload_file( \"\", header = 1,\n",
    "                         sep = ',', na_strings = ['', ' '])\n",
    "\n",
    "#raw_h2f.columns = h2o.H2OFrame.tolower().str.replace(' ', '_')\n",
    "raw_h2f.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "readData, echo=TRUE,tidy=TRUE",
    "autoscroll": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "raw_h2f[1].types\n",
    "\n",
    "filter_h2f = h2o.deep_copy(raw_h2f, 'filter_h2f')\n",
    "\n",
    "#raw_h2f.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature lists\n",
    "\n",
    "Get the numerical features, text features and categorical features in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#numerical_features = []\n",
    "\n",
    "temp_col_num = filter_h2f.columns_by_type('numeric')\n",
    "col_num  = [int(elem) for elem in temp_col_num]\n",
    "numerical_features  = filter_h2f[col_num].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The below to features are redundant\n",
    "remove_num_feature = ['sl_no','rate_recommend_products_services']\n",
    "\n",
    "numerical_features = [x for x in numerical_features \n",
    "                      if x not in remove_num_feature ]\n",
    "\n",
    "#print(\"Numeric features in data\")  \n",
    "#numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = []\n",
    "\n",
    "temp_col_num = filter_h2f.columns_by_type('categorical')\n",
    "col_num  = [int(elem) for elem in temp_col_num]\n",
    "categorical_features  = filter_h2f[col_num].columns\n",
    "\n",
    "\n",
    "    \n",
    "categorical_features = [cf for cf in categorical_features if cf not in ['nps_classification']]\n",
    "print(\"Categorical features in data\")\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = list(numerical_features)\n",
    "all_features.extend(categorical_features)\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_h2f = h2o.deep_copy(filter_h2f[all_features], 'new_h2f')\n",
    "new_h2f.na_omit()\n",
    "\n",
    "# Identify predictors and response\n",
    "response_col = 'merged_nps_classification'\n",
    "predictors = [x for x in all_features if x not in ['merged_nps_classification']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Significant Feature list\n",
    "The below features were identified as significant features after the first run of the model. These features have been used in final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "significant_predictors = [x for x in all_features if x in ['location',\n",
    "                                                             'number_of_issue_reported',\n",
    "                                                             'inclusions_exclusions_explained',\n",
    "                                                             'papers_for_the_new_contracts_received',\n",
    "                                                             'prmsso_type'\n",
    "                                                            ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "significant_predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Model Building: Using the **H2O** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train and Test split using H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_h2f,test_h2f,valid_h2f = new_h2f.split_frame(ratios=[.70, .15,], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "len(train_h2f)\n",
    "len(test_h2f)\n",
    "len(valid_h2f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Cross Validation Data as in H2O:\n",
    "\n",
    "In general, for all algos that support the nfolds parameter, H2O’s cross-validation works as follows:\n",
    "\n",
    "For example, for nfolds=5, 6 models are built. The first 5 models (cross-validation models) are built on 80% of the training data, and a different 20% is held out for each of the 5 models. Then the main model is built on 100% of the training data. This main model is the model you get back from H2O in R, Python and Flow (though the CV models are also stored and available to access later).\n",
    "\n",
    "This main model contains training metrics and cross-validation metrics (and optionally, validation metrics if a validation frame was provided). The main model also contains pointers to the 5 cross-validation models for further inspection.\n",
    "\n",
    "All 5 cross-validation models contain training metrics (from the 80% training data) and validation metrics (from their 20% holdout/validation data). To compute their individual validation metrics, each of the 5 cross-validation models had to make predictions on their 20% of of rows of the original training frame, and score against the true labels of the 20% holdout.\n",
    "\n",
    "For the main model, this is how the cross-validation metrics are computed: The 5 holdout predictions are combined into one prediction for the full training dataset (i.e., predictions for every row of the training data, but the model making the prediction for a particular row has not seen that row during training). This “holdout prediction” is then scored against the true labels, and the overall cross-validation metrics are computed.\n",
    "\n",
    "This approach has some implications. Scoring the holdout predictions freshly can result in different metrics than taking the average of the 5 validation metrics of the cross-validation models. For example, if the sizes of the holdout folds differ a lot (e.g., when a user-given fold_column is used), then the average should probably be replaced with a weighted average. Also, if the cross-validation models map to slightly different probability spaces, which can happen for small DL models that converge to different local minima, then the confused rank ordering of the combined predictions would lead to a significantly different AUC than the average.\n",
    "\n",
    "More about cross-validation at: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/cross-validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoML\n",
    "\n",
    "The H2O AutoML interface is designed to have as few parameters as possible so that all the user needs to do is point to their dataset, identify the response column and optionally specify a time constraint or limit on the number of total models trained.\n",
    "\n",
    "In both the R and Python API, AutoML uses the same data-related arguments, x, y, training_frame, validation_frame, as the other H2O algorithms. Most of the time, all you’ll need to do is specify the data arguments. You can then configure values for max_runtime_secs and/or max_models to set explicit time or number-of-model limits on your run.\n",
    "\n",
    "More about grid search at: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "AutoML performs hyperparameter search over a variety of H2O algorithms in order to deliver the best model. In AutoML, the following hyperparameters are supported by grid search. Random Forest and Extremely Randomized Trees are not grid searched (in the current version of AutoML), so they are not included in the list below.\n",
    "\n",
    "> GBM Hyperparameters: `score_tree_interval`, `histogram_type`,`ntrees`,`max_depth`, `min_rows`, `learn_rate`, `sample_rate`, `col_sample_rate`, `col_sample_rate_per_tree`, `min_split_improvement`, \n",
    "\n",
    "> GLM Hyperparameters: `alpha`, `missing_values_handling`\n",
    "\n",
    "> Deep Learning Hyperparameters: `epochs`,`adaptivate_rate`, `activation`, `rho`, `epsilon`, `input_dropout_ratio`, `hidden`, `hidden_dropout_ratios`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frames for Model\n",
    "\n",
    "If the user doesn’t specify a validation_frame, then one will be created automatically by randomly partitioning the training data. The validation frame is required for early stopping of the individual algorithms, the grid searches and the AutoML process itself.\n",
    "\n",
    "By default, AutoML uses cross-validation for all models, and therefore we can use cross-validation metrics to generate the leaderboard. If the leaderboard_frame is explicitly specified by the user, then that frame will be used to generate the leaderboard metrics instead of using cross-validation metrics.\n",
    "\n",
    "For cross-validated AutoML, when the user specifies:\n",
    "\n",
    "> * training: The training_frame is split into training (80%) and validation (20%).\n",
    "* training + leaderboard: The training_frame is split into training (80%) and validation (20%).\n",
    "* training + validation: Leave frames as-is.\n",
    "* training + validation + leaderboard: Leave frames as-is.\n",
    "\n",
    "If not using cross-validation (by setting nfolds = 0) in AutoML, then we need to make sure there is a test frame (aka. the “leaderboard frame”) to score on because cross-validation metrics will not be available. So when the user specifies:\n",
    "\n",
    ">* training: The training_frame is split into training (80%), validation (10%) and leaderboard/test (10%).\n",
    "* training + leaderboard: The training_frame is split into training (80%) and validation (20%). Leaderboard frame as-is.\n",
    "* training + validation: The validation_frame is split into validation (50%) and leaderboard/test (50%). Training frame as-is.\n",
    "* training + validation + leaderboard: Leave frames as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "?H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "caretModel, echo=TRUE, message=FALSE, warning=FALSE",
    "autoscroll": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "aml = H2OAutoML(max_models = 50, seed = 42,nfolds=0) #max_runtime_secs = 300 seed=42 max_models = 10)\n",
    "                      # exclude_algos = ['None'], balance_classes = True\n",
    "                       #seed works when max_runtime is not specified. \n",
    "\n",
    "#aml.train(x=predictors, y=response_col, training_frame= train_h2f, \n",
    "#                 validation_frame=valid_h2f)#, leaderboard_frame=test_h2f)\n",
    "\n",
    "aml.train(x=predictors, y=response_col, training_frame= train_h2f, \n",
    "                 validation_frame=valid_h2f, leaderboard_frame=test_h2f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leaderboard\n",
    "\n",
    "The AutoML object includes a “leaderboard” of models that were trained in the process, including the 5-fold cross-validated model performance (by default). The number of folds used in the model evaluation process can be adjusted using the nfolds parameter. If the user would like to score the models on a specific dataset, they can specify the leaderboard_frame argument, and then the leaderboard will show scores on that dataset instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lb = aml.leaderboard\n",
    "#lb.head(rows=lb.nrows)\n",
    "lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Models\n",
    "\n",
    "#### Stacked Ensemble\n",
    "\n",
    "To understand how the ensemble works, let's take a peek inside the Stacked Ensemble \"All Models\" model. The \"All Models\" ensemble is an ensemble of all of the individual models in the AutoML run. This is often the top performing model on the leaderboard.\n",
    "\n",
    "The leader model is stored in `aml.leader`\n",
    "\n",
    "More about AutoML @ AutoML Tutorial: https://github.com/h2oai/h2o-tutorials/tree/master/h2o-world-2017/automl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model ids for all models in the AutoML Leaderboard\n",
    "model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n",
    "\n",
    "# Get the \"All Models\" Stacked Ensemble model\n",
    "all_se = [mid for mid in model_ids if \"StackedEnsemble\" in mid]\n",
    "all_se\n",
    "se_model = h2o.get_model(all_se[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the Stacked Ensemble metalearner model if it is the leader\n",
    "#metalearner = h2o.get_model(aml.leader.metalearner()['name'])\n",
    "\n",
    "#to get the specific ensemble metalearner model (). Here it is StackedEnsemble_AllModels which is not a leader.\n",
    "metalearner = h2o.get_model(se_model.metalearner()['name'])\n",
    "metalearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the variable importance of the metalearner (combiner) algorithm in the ensemble. This shows us how much each base learner is contributing to the ensemble. The AutoML Stacked Ensembles use the default metalearner algorithm (GLM with non-negative weights), so the variable importance of the metalearner is actually the standardized coefficient magnitudes of the GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalearner.coef_norm()\n",
    "\n",
    "%matplotlib inline\n",
    "metalearner.std_coef_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "aml.leader\n",
    "#rf_model.varimp_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DRF\n",
    "\n",
    "To examine the specific model in the AutoML (e.g. DRF). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drf_model = h2o.get_model([mid for mid in model_ids if \"DRF\" in mid][0])\n",
    "drf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#drf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drf_model.actual_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBM\n",
    "\n",
    "To examine the specific model in the AutoML (e.g. GBM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gbm_model = ([x for x in model_ids if \"GBM\" in x])\n",
    "all_gbm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model = h2o.get_model(all_gbm_model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gbm_model.confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Performance\n",
    "\n",
    "To view the model performance of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1 =\"Promoter\"\n",
    "label_0 = \"Detractor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's evaluate the model performance on a test set\n",
    "predict_test_h2f = aml.leader.predict(test_h2f)\n",
    "\n",
    "#glm_predict_test_h2f = saved_model.predict(test_h2f)\n",
    "\n",
    "predict_test_h2f = h2o.H2OFrame.cbind(test_h2f[response_col],predict_test_h2f)\n",
    "    \n",
    "    \n",
    "predict_test_df =predict_test_h2f.as_data_frame()\n",
    "predict_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cut-off of 0.71 to perform the classification on test set\n",
    "Threshold `0.26083842534607526` is for max F1 score on the validation dataset. This is used by default in classifying the records in the test data\n",
    "\n",
    "Threshold `0.8214663` is for max F0.5 score on the validation dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_df['predicted'] = predict_test_df.Promoter.map(lambda x: label_1 if x > 0.8214663 else label_0)\n",
    "#glm_predict_test_df.columns.values[0] = 'actual'\n",
    "\n",
    "predict_test_df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Generic function to report the classification matrix and model statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cm(actual,predicted):\n",
    "    plt.figure(figsize=(9,9))\n",
    "    cm = metrics.confusion_matrix(actual,predicted)\n",
    "    sn.heatmap(cm, annot=True,  fmt='.0f', xticklabels = [label_0, label_1] , \n",
    "               yticklabels = [label_0, label_1],cmap = 'Blues_r')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Classification Matrix Plot', size = 15);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance (clasf_matrix):\n",
    "    measure = pd.DataFrame({\n",
    "                        'sensitivity': [round(clasf_matrix[1,1]/(clasf_matrix[1,0]+clasf_matrix[1,1]),2)], \n",
    "                        'specificity': [round(clasf_matrix[0,0]/(clasf_matrix[0,0]+clasf_matrix[0,1]),2)],\n",
    "                        'recall': [round(clasf_matrix[1,1]/(clasf_matrix[1,0]+clasf_matrix[1,1]),2)],\n",
    "                        'precision': [round(clasf_matrix[1,1]/(clasf_matrix[0,1]+clasf_matrix[1,1]),2)],\n",
    "                        'overall_acc': [round((clasf_matrix[0,0]+clasf_matrix[1,1])/\n",
    "                                              (clasf_matrix[0,0]+clasf_matrix[0,1]+clasf_matrix[1,0]+clasf_matrix[1,1]),2)]\n",
    "                       })\n",
    "    return measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_cm(predict_test_df.merged_nps_classification, predict_test_df.predict )\n",
    "draw_cm(predict_test_df.merged_nps_classification, predict_test_df.predicted )\n",
    "#draw_cm(glm_predict_test_df.actual, glm_predict_test_df.predicted )\n",
    "\n",
    "cm = metrics.confusion_matrix(predict_test_df.merged_nps_classification, predict_test_df.predicted)\n",
    "model_test_metrics = pd.DataFrame(measure_performance(cm))\n",
    "model_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#model_perff\n",
    "\n",
    "aml.leader.model_performance(test_h2f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "There are two ways to save the leader model -- binary format and MOJO format. If you're taking your leader model to production, then we'd suggest the MOJO format since it's optimized for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "#model_path = h2o.save_model(model=aml.leader, path=\"/Users/Rahul/Documents/\", force=True)\n",
    "#print(model_path)\n",
    "\n",
    "#mojo\n",
    "#aml.leader.download_mojo(path=\"/Users/Rahul/Documents/\")\n",
    "\n",
    "\n",
    "# load the model\n",
    "#saved_model = h2o.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# THANK YOU\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "Rmd_header": {
   "author": "Kumar Rahul",
   "date": "9 September 2016",
   "output": "word_document",
   "title": "Logistic Regression using Caret Package"
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
